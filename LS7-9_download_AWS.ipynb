{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Landsat 7-9 image subsets over an AOI using AWS\n",
    "\n",
    "_Last modified 2023-11-1._\n",
    "\n",
    "This script is run to download Landsat images over the glaciers available through the Amazon Web Services (AWS) s3 bucket. The workflow is streamlined to analyze images for 10s to 100s of glaciers, specifically, the marine-terminating glaciers along the periphery of Greenland. Sections of code that may need to be modified are indicated as below:\n",
    "\n",
    "    ##########################################################################################\n",
    "\n",
    "    code to modify\n",
    "\n",
    "    ##########################################################################################\n",
    "\n",
    " \n",
    "### First, configure your AWS profile to access the Landsat images on the s3 bucket:\n",
    "\n",
    "Follow instructions at https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html to get required __aws command line software__.\n",
    "\n",
    "Set up your AWS profile with a payment account. Then configure it to your machine following these steps:\n",
    "\n",
    "    aws configure --profile terminusmapping\n",
    "    \n",
    "Enter in your credentials, which will be stored locally on your computer. The Boto3 package will be used to access your credentials without leaking your keys. **Protect your AWS access keys!** DO NOT print anything that involves your ACCESS_KEY and SECRET_KEY. GitGuardian may help track any leaked keys. In order to use these credentials to download subsets of images from AWS using vsi3, **GDAL version 3.2. or newer must be installed**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS settings\n",
    "import boto3\n",
    "import boto3.session\n",
    "\n",
    "cred = boto3.Session(profile_name='terminusmapping').get_credentials()\n",
    "ACCESS_KEY = cred.access_key\n",
    "SECRET_KEY = cred.secret_key\n",
    "SESSION_TOKEN = cred.token  ## optional\n",
    "\n",
    "\n",
    "s3client = boto3.client('s3', \n",
    "                        aws_access_key_id = ACCESS_KEY, \n",
    "                        aws_secret_access_key = SECRET_KEY, \n",
    "#                         aws_session_token = SESSION_TOKEN\n",
    "                       )\n",
    "\n",
    "######################################################################################\n",
    "# path to the collection on AWS usgs-landsat s3 bucket:\n",
    "collectionpath = 'collection02/level-1/standard/' # collection 2 level 1 data being used (contains B8)\n",
    "######################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Set-up: import packages and set paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import subprocess\n",
    "import os\n",
    "import shutil\n",
    "import datetime\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "# geospatial packages\n",
    "import fiona\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, Point, LineString\n",
    "import shapely\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "# Enable fiona KML file reading driver\n",
    "fiona.drvsupport.supported_drivers['KML'] = 'rw'\n",
    "\n",
    "# distance function\n",
    "def distance(x1, y1, x2, y2):\n",
    "    dist = (((x2-x1)**2)+((y2-y1)**2))**(1/2)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define paths, satellites, geographic projections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "# ADJUST THESE VARIABLES:\n",
    "# # basepath = '/Users/jukesliu/Documents/TURNER/DATA/shapefiles_gis/' # folder containing the glacier shapefile(s)\n",
    "# # downloadpath ='/Users/jukesliu/Documents/TURNER/DATA/IMAGERY/LSimages/' # folder to eventually contain downloaded Landsat images\n",
    "# basepath = '/Users/jukesliu/Documents/PLANETSCOPE_VELOCITIES/' # folder containing the glacier shapefile(s)\n",
    "# downloadpath ='/Volumes/JUKES_EXT/CautoRIFT_sites/LS_LO/' # folder to eventually contain downloaded Landsat images\n",
    "basepath = '/Users/ellynenderlin/Research/NASA_CryoIdaho/glaciers/Wolverine/AOIs/' # folder containing the glacier shapefile(s)\n",
    "downloadpath ='/Users/ellynenderlin/Research/NASA_CryoIdaho/glaciers/Wolverine/images/LC/' # folder to eventually contain downloaded Landsat images\n",
    "\n",
    "sats = ['L8','L9'] # names of landsats to download images from ('L7' for Landsat 7 or 'L8' or both)\n",
    "# sats = ['L7']\n",
    "L9_yrs = np.arange(2018,2025).astype(str) # set target years for L9: 2020-2021\n",
    "L8_yrs = np.arange(2018,2025).astype(str) # set target years for L8: 2013-2021\n",
    "L7_yrs = np.arange(1999,2003).astype(str) # set target years for L7: 1999-2003\n",
    "L9_bands = [8] # panchromatic band for L9\n",
    "L8_bands = [8] # panchromatic band for L8\n",
    "L7_bands = [8] # panchromatic band for L7\n",
    "######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in AOI Box shapefile (should be a rectangle, not RGI polgyon) and determine source_srs ####################################\n",
    "# AOIpath = basepath+'LO_Box_WGS.shp'\n",
    "AOIpath = basepath+'Wolverine-2018-box-UTM.shp'\n",
    "# this path is also where the UTM reprojections of the AOI shp will be stored\n",
    "AOI = fiona.open(AOIpath).next()\n",
    "aoi = AOI['geometry']['coordinates'][0]\n",
    "print(aoi)\n",
    "\n",
    "# automatically extract the source coordinate reference system from the input file\n",
    "from fiona.crs import to_string\n",
    "with fiona.open(AOIpath) as colxn:\n",
    "    source_srs_str = to_string(colxn.crs)\n",
    "source_srs = source_srs_str[6:]\n",
    "print(source_srs)\n",
    "# manually enter source_srs if needed\n",
    "# source_srs = '32606' # EPSG code for the current projection of the glacier shapefile(s)\n",
    "\n",
    "# Reproject to WGS if in a different srs \n",
    "if not source_srs.endswith('4326'):\n",
    "    print('reprojecting file')\n",
    "    #IF GENERIC NAME: \"BoxID\" with number\n",
    "    # boxespath = basepath+\"Box\"+BoxID+\"/Box\"+BoxID # access the BoxID folders created \n",
    "    # rp = \"ogr2ogr -f 'ESRI Shapefile' -t_srs EPSG:4326 -s_srs EPSG:\"+source_srs+\" \"\n",
    "    # rp +=boxespath+\"_WGS.shp \"+boxespath+\".shp\"\n",
    "    #IF CUSTOM NAME (NOTE: customized indexing based on input AOI name)\n",
    "    if source_srs.startswith('epsg'):\n",
    "        rp = \"ogr2ogr -f 'ESRI Shapefile' -t_srs EPSG:4326 -s_srs \"+source_srs+\" \"\n",
    "    else:\n",
    "        rp = \"ogr2ogr -f 'ESRI Shapefile' -t_srs EPSG:4326 -s_srs EPSG:\"+source_srs+\" \"\n",
    "    rp +=AOIpath[:-7]+\"WGS.shp \"+AOIpath \n",
    "    \n",
    "    # check the command and run it\n",
    "    print(\"Command:\", rp) # check command\n",
    "    subprocess.run(rp, shell=True, check=True) # run the command on terminal\n",
    "    # load the new AOI\n",
    "    AOIpath = AOIpath[:-7]+'WGS.shp'\n",
    "    AOI = fiona.open(AOIpath).next()\n",
    "    aoi = AOI['geometry']['coordinates'][0]\n",
    "    # print(aoi) #check that coordinates are cartesian and make sense\n",
    "\n",
    "# # if an error is produced, check the error output on the terminal window that runs this notebook\n",
    "######################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2) Find all the Landsat footprints that overlap the glaciers\n",
    "\n",
    "This step requires the **WRS-2_bound_world_0.kml** file containing the footprints of all the Landsat scene boundaries available through the USGS (https://www.usgs.gov/media/files/landsat-wrs-2-scene-boundaries-kml-file). Place this file in your base directory (basepath). \n",
    "\n",
    "To check if they overlap the glacier terminus box shapefiles, the box shapefiles must be in WGS84 coordinates (ESPG: 4326). If they are not yet, we use the following GDAL command to reproject them into WGS84:\n",
    "\n",
    "        ogr2ogr -f \"ESRI Shapefile\" -t_srs EPSG:NEW_EPSG_NUMBER -s_srs EPSG:OLD_EPSG_NUMBER out.shp in.shp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "# open the kml file with the Landsat path, row footprints:\n",
    "# WRS = fiona.open('/Volumes/JUKES_EXT/WRS-2_bound_world_0.kml', driver='KML') # check the path to the world bounds file\n",
    "WRS = fiona.open(basepath+'WRS-2_bound_world_0.kml', driver='KML') # check the path to the world bounds file\n",
    "print('Landsat footprint file opened.')\n",
    "######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Grab the WGS84 coordinates of the AOI\n",
    "points = [] # to hold the box vertices\n",
    "# read coordinates and convert to a shapely object\n",
    "for coord_pair in list(aoi): \n",
    "    lat = coord_pair[0]; lon = coord_pair[1]        \n",
    "    point = shapely.geometry.Point(lat, lon) # create shapely point \n",
    "    points.append(point) # append to points list\n",
    "print(\"coordinates recorded.\") # keep track of progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []; rows = []; boxes = [] # create lists to hold the paths and rows and BoxIDs\n",
    "\n",
    "#loop through all Landsat scenes (path, row footprints)\n",
    "for feature in WRS:\n",
    "    # create shapely polygons from the Landsat footprints\n",
    "    coordinates = feature['geometry']['coordinates'][0]\n",
    "    coords = [xy[0:2] for xy in coordinates]\n",
    "    pathrow_poly = Polygon(coords)\n",
    "    \n",
    "    # grab the path and row name from the WRS kml file:\n",
    "    pathrowname = feature['properties']['Name']  \n",
    "    path = pathrowname.split('_')[0]; row = pathrowname.split('_')[1]\n",
    "#     print(path, row)\n",
    "      \n",
    "    box_points_in = 0 # counter for number of box_points in the pathrow_geom:\n",
    "    for i in range(0, len(points)):\n",
    "        point = points[i]\n",
    "        if point.within(pathrow_poly): # if the pathrow shape contains the point\n",
    "            box_points_in = box_points_in+1 # append the counter\n",
    "    if box_points_in == 5: # if all box vertices are inside the footprint, save the path, row, BoxID\n",
    "        paths.append('%03d' % int(path))\n",
    "        rows.append('%03d' % int(row))\n",
    "\n",
    "# Store in dataframe\n",
    "boxes_pr_df = pd.DataFrame(list(zip(paths, rows)), columns=['Path', 'Row'])\n",
    "boxes_pr_df # display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Download metadata files from AWS s3 for overlapping Landsat scenes\n",
    "     \n",
    "The syntax for listing the Collection 2 Landsat image files AWS s3 bucket is as follows:\n",
    "\n",
    "    aws s3 ls --profile terminusmapping --request-payer requester s3://usgs-landsat/collection02/level-2/standard/oli-tirs/yyyy/path/row/LC08_LS2R_pathrow_yyyyMMdd_yyyyMMdd_02_T1/ \n",
    "    \n",
    "__NOTE: Including the --request-payer requester as part of this line indicates that the referenced user will be charged for data download.__\n",
    "\n",
    "We can use the paths and rows in the dataframe to access the full Landsat scene list and the corresponding metdata files. Read https://docs.opendata.aws/landsat-pds/readme.html to learn more.\n",
    "    \n",
    "The metadata files will be downloaded into folders corresponding to the Landsat footprint, identified by the Path Row numbers:\n",
    "    \n",
    "    aws s3api get-object --bucket usgs-landsat --key collection02/level-2/standard/oli-tirs/yyyy/path/row/LC08_L2SP_pathrow_yyyyMMdd_yyyyMMdd_02_T1/LC08_L2SP_pathrow_yyyyMMdd_yyyyMMdd_02_T1_MTL.txt  --request-payer requester LC08_L2SP_pathrow_yyyyMMdd_yyyyMMdd_02_T1_MTL.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loop through the dataframe containing overlapping path, row info:\n",
    "for index, row in boxes_pr_df.iterrows():\n",
    "    p = row['Path']; r = row['Row']; folder_name = 'Path'+p+'_Row'+r+'_c2' # folder name\n",
    "    bp_out = downloadpath+folder_name+'/' # output path for the downloaded files\n",
    "    print(\"Downloaded metadata files are stored in:\",bp_out)\n",
    "    \n",
    "    # create Path_Row folders if they don't exist already\n",
    "    if os.path.exists(bp_out):\n",
    "        print(folder_name, \" exists already, skip directory creation\")\n",
    "    else:\n",
    "        os.mkdir(bp_out)\n",
    "        print(folder_name+\" directory made\")\n",
    "    \n",
    "    for sat in sats: # for each satellite\n",
    "        if sat == 'L8':\n",
    "            collectionfolder = 'oli-tirs/'; years = L8_yrs; prefix='LC08' # set folder, years, file prefix\n",
    "        elif sat == 'L7':\n",
    "            collectionfolder = 'etm/'; years = L7_yrs; prefix='LE07' # set folder, years, file prefix\n",
    "        elif sat == 'L9':\n",
    "            collectionfolder = 'oli-tirs/'; years = L8_yrs; prefix='LC09' # set folder, years, file prefix\n",
    "            \n",
    "        # loop through years\n",
    "        for year in years:\n",
    "            # grab list of images in each year, path, row folder\n",
    "            find_imgs = 'aws s3 ls --profile terminusmapping --request-payer requester s3://usgs-landsat/'\n",
    "            find_imgs += collectionpath+collectionfolder\n",
    "            find_imgs += year+'/'+p+'/'+r+'/'\n",
    "\n",
    "            if subprocess.run(find_imgs,shell=True).returncode != 0:\n",
    "                print('No results found for '+collectionpath+collectionfolder+year+'/'+p+'/'+r+'/')\n",
    "                results = [] # empty results\n",
    "            else:\n",
    "                result = subprocess.check_output(find_imgs,shell=True) # grab the avilable images\n",
    "                results = result.split() # split string\n",
    "            \n",
    "            imagenames = []\n",
    "            for line in results: # loop through strings\n",
    "                line = str(line)\n",
    "                if prefix in line and 'T1' in line: # find just the Tier-1 images\n",
    "                    imgname = line[2:-2]; imagenames.append(imgname)\n",
    "\n",
    "            # download the metadata (MTL.txt) file if it doesn't exist\n",
    "            for imgname in imagenames:\n",
    "                if not os.path.exists(bp_out+imgname+'_MTL.txt'): # check in output directory\n",
    "                    command = 'aws s3api get-object --bucket usgs-landsat --key '+collectionpath+collectionfolder\n",
    "                    command += year+'/'+p+'/'+r+'/'\n",
    "                    command += imgname+'/'+imgname+'_MTL.txt'\n",
    "                    command += ' --profile terminusmapping --request-payer requester '\n",
    "                    command += bp_out+imgname+'_MTL.txt'\n",
    "                    print('Downloading', imgname+'_MTL.txt')\n",
    "                    subprocess.run(command,shell=True,check=True)\n",
    "                else:\n",
    "                    print(imgname+'_MTL.txt exists. Skip.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Download QAPIXEL band over terminus box to determine cloud cover\n",
    "\n",
    "If the terminus box shapefiles were not originally in UTM projection, will need to reproject them into UTM to match the Landsat projection. The code automatically finds the UTM zones from the metadata files and fills in the following syntax to reproject:\n",
    "    \n",
    "    ogr2ogr -f \"ESRI Shapefile\" -t_srs EPSG:326zone output.shp input.shp\n",
    "    \n",
    "#### If the terminus box shapefiles are already in UTM projection, skip the following cell and rename the files to end with \"\\_UTM\\_##.shp\" where ## corresponds to the zone number (e.g., \"\\_UTM\\_07.shp\", \"\\_UTM\\_21.shp\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If pyproj is not showing up:\n",
    "# os.environ['PROJ_LIB'] = '/Users/jukesliu/opt/anaconda3/envs/autoterm_env/share/proj'\n",
    "os.environ['PROJ_LIB'] = '/Users/ellynenderlin/micromamba/envs/preautorift/share/proj'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zones = {} # initialize dictionary to hold UTM zone for each Landsat scene path row\n",
    "zone_list = [] # list of zones\n",
    "\n",
    "# Loop through all scenes:\n",
    "for index, row in boxes_pr_df.iterrows():\n",
    "    p = row['Path']; r = row['Row']; folder_name = 'Path'+p+'_Row'+r+'_c2' # Landsat path and row\n",
    "    pr_folderpath = downloadpath+folder_name+'/' # path to the downloaded metadata files\n",
    "    \n",
    "    if len(os.listdir(pr_folderpath)) > 0: # if there are files in the folder\n",
    "        # grab UTM Zone from the first metadata file\n",
    "        mtl_scene = glob.glob(pr_folderpath+'*_MTL.txt')[0]\n",
    "        mtl = open(mtl_scene, 'r')\n",
    "        \n",
    "        # loop through lines in the metadata file to find the UTM ZONE\n",
    "        for line in mtl:  \n",
    "            variable = line.split(\"=\")[0] # grab the variable name\n",
    "            if (\"UTM_ZONE\" in variable):\n",
    "                zone = '%02d' % int(line.split(\"=\")[1][1:-1]) # grab the 2-digit zone number\n",
    "                zones.update({folder_name: zone}); zone_list.append(zone) # add to zone lists\n",
    "                break\n",
    "                \n",
    "        # reproject shapefile(s) into UTM\n",
    "        zone = zones[folder_name]\n",
    "        rp_shp = 'ogr2ogr -f \"ESRI Shapefile\" '+AOIpath[:-4]+'_UTM_'+zone+'.shp '+AOIpath\n",
    "        rp_shp += ' -t_srs EPSG:326'+zone\n",
    "        subprocess.run(rp_shp, shell=True,check=True)\n",
    "        \n",
    "    else: # if no files in folder, zone = nan, must fill in manually\n",
    "        zone_list.append(np.nan)\n",
    "        \n",
    "boxes_pr_df['Zone'] = zone_list # add to the path row dataframe\n",
    "boxes_pr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use GDAL and __vsi3__ link to download subset of the quality band we will use to determine cloud cover over the terminus:\n",
    "\n",
    "    gdalwarp -cutline path_to_shp.shp -crop_to_cutline /vsi3/usgs-landsat/collection02/level-1/standard/oli-tirs/yyyy/path/row/scene/scene_QA_PIXEL.TIF path_to_subset_QA_PIXEL.TIF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loop through all scenes:\n",
    "for index, row in boxes_pr_df.iterrows():\n",
    "    p = row['Path']; r = row['Row']; zone = row['Zone'] # grab path, row, zone\n",
    "    folder_name = 'Path'+p+'_Row'+r+'_c2'\n",
    "    pr_folderpath = downloadpath+folder_name+'/' # path to the downloaded metadata files\n",
    "    pathtoshp_rp = AOIpath[:-4]+'_UTM_'+zone # path to the UTM projected box shapefile\n",
    "\n",
    "    files = os.listdir(pr_folderpath) # grab the names of the Landsat scenes\n",
    "    \n",
    "    # for all files in the path row folders\n",
    "    for file in files:\n",
    "        scene = file[:40] # slice the filename to grab the scene name\n",
    "\n",
    "        if scene.startswith('L') and 'T1' in scene: # L1TP scenes\n",
    "            scene_year = scene[17:21] # grab the year from the scene name\n",
    "            \n",
    "            if scene.startswith('LC08') or scene.startswith('LC09'):\n",
    "                collectionfolder='oli-tirs/'\n",
    "            elif scene.startswith('LE07'):\n",
    "                collectionfolder='etm/'\n",
    "                \n",
    "            # set path to the QA pixel Landsat files\n",
    "            pathtoQAPIXEL='/vsis3/usgs-landsat/'+collectionpath+collectionfolder\n",
    "            pathtoQAPIXEL+=scene_year+'/'\n",
    "            pathtoQAPIXEL+=p+'/'+r+'/'\n",
    "            pathtoQAPIXEL+=scene+'/'+scene+\"_QA_PIXEL.TIF\"\n",
    "            \n",
    "            # set path to the subset QA pixel files inside the path row folders\n",
    "            subsetout = pr_folderpath+scene+'_QA_PIXEL.TIF' \n",
    "            \n",
    "            # if the file hasn't already been downloaded\n",
    "            if not os.path.exists(subsetout): #and not scene_year.startswith('202'): # EXCLUDE CERTAIN YEARS\n",
    "                print('Downloading', scene)\n",
    "                # construct download command\n",
    "                QAPIXEL_dwnld_cmd='gdalwarp -overwrite -cutline '+pathtoshp_rp+'.shp -crop_to_cutline '\n",
    "                QAPIXEL_dwnld_cmd+= pathtoQAPIXEL+' '+subsetout\n",
    "                QAPIXEL_dwnld_cmd+=' --config AWS_REQUEST_PAYER requester --config AWS_REGION us-west-2'\n",
    "                QAPIXEL_dwnld_cmd+=' --config AWS_SECRET_ACCESS_KEY '+SECRET_KEY\n",
    "                QAPIXEL_dwnld_cmd+=' --config AWS_ACCESS_KEY_ID '+ACCESS_KEY\n",
    "\n",
    "                subprocess.run(QAPIXEL_dwnld_cmd, shell=True, check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Download non-cloudy Landsat images from AWS\n",
    "\n",
    "To remove cloudy images, we will find the number of pixels in our terminus box that exceed a threshold value in the QA_PIXEL band corresponding to cloud and cloud shadow likelihood. If the fraction of cloudy pixels with values is above the threshold, we won't download the image. See Landsat Collection 2 Level 2 Science Product Guide for [Landsat 8](https://d9-wret.s3.us-west-2.amazonaws.com/assets/palladium/production/s3fs-public/media/files/LSDS-1619_Landsat-8-9-C2-L2-ScienceProductGuide-v4.pdf) and [Landsat 7](https://d9-wret.s3.us-west-2.amazonaws.com/assets/palladium/production/s3fs-public/atoms/files/LSDS-1618_Landsat-4-7_C2-L2-ScienceProductGuide-v3.pdf) more information on how the QA_PIXEL threshold values are chosen.\n",
    "\n",
    "Additionally, we remove images that are primarily black (fill value of 1 in QA_PIXEL band). This ensures that the scenes that cut off halfway across the glacier are not included in further analysis. The fill percent threshold may need to be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "# These are the recommended values based on the Collection 2 Level 2 Science Product Guide.\n",
    "# Adjust thresholds here:\n",
    "QAPIXEL_thresh_lower_L7 = 5696.0 # minimum QA pixel value threshold to be considered cloud for L7 images\n",
    "QAPIXEL_thresh_upper_L7 = 7568.0 # maximum QA pixel value threshold to be considered cloud for L7 images\n",
    "\n",
    "# Landsat 8 requires two lower thresholds, we selct between 22280 and 24472 and above 54596\n",
    "QAPIXEL_thresh_lower_L8 = 22080.0 # minimum QA pixel value threshold to be considered cloud for L8 images\n",
    "QAPIXEL_thresh_upper_L8 = 30048.0 # maximum QA pixel value threshold to be considered cloud for L8 images\n",
    "QAPIXEL_thresh2_lower_L8 = 54596.0 # 2nd minimum QA pixel value threshold to be considered cloud for L8 images\n",
    "\n",
    "cpercent_thresh = 50.0 # maximum cloud cover % in terminus box\n",
    "fpercent_thresh = 60.0 # maximum fill % in terminus box\n",
    "######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download images that pass these thresholds:\n",
    "for index, row in boxes_pr_df.iterrows():\n",
    "    # grab paths\n",
    "    p = row['Path']; zone = row['Zone']; r = row['Row']\n",
    "    folder_name = 'Path'+p+'_Row'+r+'_c2'\n",
    "    pr_folderpath = downloadpath+folder_name+'/'\n",
    "    bp_out = downloadpath\n",
    "    \n",
    "    print(pr_folderpath)\n",
    "    \n",
    "    # path to the shapefile covering the region that will be downloaded\n",
    "    pathtobuffer = AOIpath[:-4]+'_UTM_'+zone+'.shp' # just the box\n",
    "    \n",
    "    for scene in os.listdir(pr_folderpath):\n",
    "        if scene.startswith('L') and scene.endswith(\".TIF\") and 'T1' in scene and 'L1TP' in scene: # For Tier-1 images\n",
    "            scene = scene[:40] # scene name\n",
    "            year = scene[17:21] # grab acquisition year\n",
    "            \n",
    "            QApixelpath = pr_folderpath+scene+'_QA_PIXEL.TIF' # path to QA_PIXEL file\n",
    "            subsetQApixel = mpimg.imread(QApixelpath) # read in QAPIXEL file as numpy array\n",
    "            totalpixels = subsetQApixel.shape[0]*subsetQApixel.shape[1] # count total number of pixels\n",
    "            \n",
    "            if scene.startswith(\"LC08\") or scene.startswith('LC09'): # Landsat 8 or Landsat 9\n",
    "                collectionfolder = 'oli-tirs/'; bands = L8_bands; \n",
    "                # countcloudy pixels based on thresholds:\n",
    "                cloudQApixel = subsetQApixel[((subsetQApixel >= QAPIXEL_thresh_lower_L8) &\n",
    "                                             (subsetQApixel < QAPIXEL_thresh_upper_L8) | \n",
    "                                             (subsetQApixel >= QAPIXEL_thresh2_lower_L8))]\n",
    "                \n",
    "            elif scene.startswith(\"LE07\"): # Landsat 7\n",
    "                collectionfolder = 'etm/'; bands = L7_bands\n",
    "                # countcloudy pixels based on thresholds:\n",
    "                cloudQApixel = subsetQApixel[((subsetQApixel >= QAPIXEL_thresh_lower_L7) & \n",
    "                                             (subsetQApixel < QAPIXEL_thresh_upper_L7))]\n",
    " \n",
    "            # calculate percentages of cloud and fill pixels\n",
    "            fillQApixel = subsetQApixel[subsetQApixel < 2.0] # fill pixels (value = 0 or 1)\n",
    "            cloudpixels = len(cloudQApixel); fillpixels = len(fillQApixel) # count the cloudy and fill pixels\n",
    "            cloudpercent = int(float(cloudpixels)/float(totalpixels)*100) # calculate percent cloudy\n",
    "            fillpercent = int(float(fillpixels)/float(totalpixels)*100) # calculate percent fill\n",
    "            \n",
    "            # evaluate thresholds\n",
    "            if cloudpercent <= cpercent_thresh and fillpercent <= fpercent_thresh:\n",
    "                # download the bands for that scene into your scene folders:\n",
    "                for band in bands:\n",
    "                        band = str(band) # string format\n",
    "                        \n",
    "                        # input path to your bands in AWS:\n",
    "                        pathin = '/vsis3/usgs-landsat/'+collectionpath+collectionfolder+year+'/'+p+\"/\"+r+\"/\"+scene+\"/\"+scene+\"_B\"+band+\".TIF\"\n",
    "                        \n",
    "                        outfilename = scene+\"_B\"+band+'.TIF' # output file name\n",
    "                        pathout = downloadpath+outfilename # full output file path\n",
    "                        \n",
    "                        # if the file hasn't already been downloaded\n",
    "                        if not os.path.exists(pathout):\n",
    "                            # download\n",
    "                            download_cmd = 'gdalwarp -overwrite -cutline '+pathtobuffer+' -crop_to_cutline '+pathin+' '+pathout\n",
    "                            download_cmd+=' --config AWS_REQUEST_PAYER requester --config AWS_REGION us-west-2'\n",
    "                            download_cmd+=' --config AWS_SECRET_ACCESS_KEY '+SECRET_KEY\n",
    "                            download_cmd+=' --config AWS_ACCESS_KEY_ID '+ACCESS_KEY   \n",
    "                            print('Downloading:', outfilename)\n",
    "                            subprocess.run(download_cmd, shell=True, check=True)\n",
    "                        else:\n",
    "                            print(outfilename, 'exists')\n",
    "            else:\n",
    "                print(scene, 'failed cloud & fill thresholds: Cloud % ', cloudpercent, 'Fill %', fillpercent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Delete all quality band files (*QA_PIXEL.TIF) to save space\n",
    "\n",
    "These files will not be needed after the download step, so they can be removed to save space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab all path row folder names from boxes_pr_df:\n",
    "paths = boxes_pr_df['Path']; rows = boxes_pr_df['Row']\n",
    "for a in range(0, len(paths)): # look in each path row folder\n",
    "    folder_name = 'Path'+paths[a]+'_Row'+rows[a]+'_c2'\n",
    "    folderpath = downloadpath+folder_name+'/'\n",
    "\n",
    "    # remove all files with QA_PIXEL in the name\n",
    "    for file in os.listdir(folderpath):\n",
    "        if 'QA_PIXEL' in file:\n",
    "            os.remove(folderpath+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
